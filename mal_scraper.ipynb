{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_page = requests.get('https://myanimelist.net/anime/season')\n",
    "mal_tree = html.fromstring(mal_page.content)\n",
    "new_tv_urls = []\n",
    "continuing_tv_urls = []\n",
    "ONA_urls = []\n",
    "OVA_urls = []\n",
    "movie_urls = []\n",
    "special_urls = []\n",
    "urls_lst = [[] for i in range(6)]\n",
    "names_lst = [[] for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping new tv series urls\n",
    "url_count = 2\n",
    "while True:\n",
    "    true_count = 0\n",
    "    for i in np.arange(1,7):\n",
    "        url_path = '//*[@id=\"content\"]/div[4]/div['+str(i)+']/div['+str(url_count)+']/div[1]/div[1]/p/a/@href'\n",
    "        name_path = '//*[@id=\"content\"]/div[4]/div['+str(i)+']/div['+str(url_count)+']/div[1]/div[1]/p/a/text()'\n",
    "        url = mal_tree.xpath(url_path)\n",
    "        name = mal_tree.xpath(name_path)\n",
    "        if url != []:\n",
    "            urls_lst[i-1].append(url[0])\n",
    "            names_lst[i-1].append(name[0])\n",
    "            true_count += 1\n",
    "    url_count += 1\n",
    "    if true_count == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_names = []\n",
    "for names in names_lst:\n",
    "    for name in names:\n",
    "        unrolled_names.append(name)\n",
    "unrolled_urls = []\n",
    "for urls in urls_lst:\n",
    "    for url in urls:\n",
    "        unrolled_urls.append(url)\n",
    "type_lst = []\n",
    "for i in np.arange(6):\n",
    "    for j in np.arange(len(urls_lst[i])):\n",
    "        if i == 0:\n",
    "            type_lst.append('TV (New)')\n",
    "        elif i == 1:\n",
    "            type_lst.append('TV (Continuing)')\n",
    "        elif i == 2:\n",
    "            type_lst.append('ONA')\n",
    "        elif i == 3:\n",
    "            type_lst.append('OVA')\n",
    "        elif i == 4:\n",
    "            type_lst.append('Movie')\n",
    "        else:\n",
    "            type_lst.append('Special')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping info from urls\n",
    "info = [[] for i in range(5)]\n",
    "for url in unrolled_urls:\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',\n",
    "        'scheme': 'https',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "        'accept-encoding': 'gzip, deflate, br',\n",
    "        'accept-language': 'en-US,en;q=0.9,und;q=0.8',\n",
    "        'upgrade-insecure-requests': '1'\n",
    "    }\n",
    "    url = str(url)\n",
    "    anime_page = requests.get(url, headers=headers)\n",
    "    anime_tree = html.fromstring(anime_page.content)\n",
    "    score_path = '//*[@id=\"content\"]/table/tr/td[2]/div[1]/table/tr[1]/td/div[1]/div[1]/div[1]/div[1]/div[1]/text()'\n",
    "    rank_path = '//*[@id=\"content\"]/table/tr/td[2]/div[1]/table/tr[1]/td/div[1]/div[1]/div[1]/div[1]/div[2]/span[1]/strong/text()'\n",
    "    popularity_path = '//*[@id=\"content\"]/table/tr/td[2]/div[1]/table/tr[1]/td/div[1]/div[1]/div[1]/div[1]/div[2]/span[2]/strong/text()'\n",
    "    members_path = '//*[@id=\"content\"]/table/tr/td[2]/div[1]/table/tr[1]/td/div[1]/div[1]/div[1]/div[1]/div[2]/span[3]/strong/text()'\n",
    "    info[0].append(anime_tree.xpath(score_path))\n",
    "    time.sleep(0.5)\n",
    "    info[1].append(anime_tree.xpath(rank_path))\n",
    "    time.sleep(0.5)\n",
    "    info[2].append(anime_tree.xpath(popularity_path))\n",
    "    time.sleep(0.5)\n",
    "    info[3].append(anime_tree.xpath(members_path))\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning data\n",
    "cleaned_info = [[] for i in range(4)]\n",
    "for i in np.arange(5):\n",
    "    for item in info[i]:\n",
    "        if item == []:\n",
    "            cleaned_info[i].append('N/A')\n",
    "        else:\n",
    "            cleaned_info[i].append(item[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dct = {\n",
    "    'Name': unrolled_names,\n",
    "    'Type': type_lst,\n",
    "    'Score': cleaned_info[0],\n",
    "    'Rank': cleaned_info[1],\n",
    "    'Popularity': cleaned_info[2],\n",
    "    'Members': cleaned_info[3],\n",
    "    'URL': unrolled_urls\n",
    "}\n",
    "df = pd.DataFrame(data=data_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to CSV\n",
    "df.to_csv('current_season_anime.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
